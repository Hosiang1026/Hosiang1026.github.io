<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="Access-Control-Allow-Origin" content="*"><script type="text/javascript">var start_time=(new Date).getTime()</script><meta http-equiv="Cache-Control" content="no-cache"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="viewport" content="initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="google-site-verification" content="9Wt3lmnrT2bfjaHla5s16adhvcQcBHO7PtNf97n5Od0"><meta name="baidu-site-verification" content="RYlYHsgO7Y"><meta name="sogou_site_verification" content="MSmrTj7lHV"><meta name="shenma-site-verification" content="e4428ccce0f22fad61c3716193bf4bd7_1571416388"><meta name="baidu_union_verify" content="584bb6b4bd24108df082b6d20c7aa9be"><title>推荐系列- -大规模训练-大规模训练的技术挑战 | 狂欢马克思</title><meta name="keywords" content="分享技术经验与交流"><meta name="description" content="&amp;emsp;&amp;emsp;@ 不愿透露姓名的小 P 同学 0 前言 本次分享是大规模训练技术系列的第一篇，主要包括两个部分： 大规模训练技术的意义 大规模训练的技术挑战 1 大规模训练技术的意义 1.1 训练的精度极限 …"><meta property="og:type" content="article"><meta property="og:title" content="推荐系列- -大规模训练-大规模训练的技术挑战"><meta property="og:url" content="https://www.hosiang.cn/c3019c25/index.html"><meta property="og:site_name" content="狂欢马克思"><meta property="og:description" content="&amp;emsp;&amp;emsp;@ 不愿透露姓名的小 P 同学 0 前言 本次分享是大规模训练技术系列的第一篇，主要包括两个部分： 大规模训练技术的意义 大规模训练的技术挑战 1 大规模训练技术的意义 1.1 训练的精度极限 …"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2022-05-10T21:14:30.000Z"><meta property="article:modified_time" content="2022-05-11T09:37:52.902Z"><meta property="article:author" content="Howe Hsiang"><meta property="article:tag" content="Popular"><meta name="twitter:card" content="summary"><link rel="icon" href="/images/favicon.ico"><link href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="https://cdn.bootcss.com/font-awesome-animation/0.1.0/font-awesome-animation.min.css" rel="stylesheet"><link rel="stylesheet" href="/css/style.css"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?3cd8fa109426bf3f10bd5c362175bace";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script></script><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 6.2.0"></head><script src="/js/jquery.min.js"></script><script src="/js/pace.min.js"></script><script src="/js/crypto-js.js"></script><script src="/js/cookie.js"></script><script src="/js/calendar.js"></script><script src="/js/festival.js"></script><script>getFestival()</script><body><script>window.onload=function(){document.getElementById("TimeShow").innerHTML="本站耗时 "+((new Date).getTime()-start_time)/1e3+" 秒 "}</script><div id="container"><header id="header"><div id="banner"></div><div id="header-outer"><div id="header-menu" class="header-menu-pos animated"><div class="header-menu-container"><a href="/" class="left"><span class="site-title">狂欢马克思</span></a><nav id="header-menu-nav" class="right"><a href="/" rel="external nofollow"><i class="fa fa-home"></i> <span>主页</span> </a><a href="/archives" rel="external nofollow"><i class="fa fa-archive"></i> <span>归档</span> </a><a href="/gitbook" rel="external nofollow"><i class="fa fa-columns"></i> <span>笔记</span> </a><a href="/music" rel="external nofollow"><i class="fa fa-music"></i> <span>音乐</span> </a><a href="/photo" rel="external nofollow"><i class="fa fa-picture-o"></i> <span>相册</span> </a><a href="/collection" rel="external nofollow"><i class="fa fa-envira"></i> <span>收藏</span> </a><a href="/about" rel="external nofollow"><i class="fa fa-user"></i> <span>关于</span></a></nav><a class="mobile-header-menu-button"><i class="fa fa-bars"></i></a></div></div><div id="header-row"><div id="logo"><a href="/"><img src="/../images/logo.png" alt="logo"></a></div><div class="header-info"><div id="header-title"><h2>狂欢马克思</h2></div><div id="header-description"><h3>专注Web开发</h3></div></div><nav class="header-nav"><div class="social"><a title="Github" target="_blank" href="//github.com/Hosiang1026" rel="external nofollow"><i class="fa fa-github fa-2x"></i></a> <a title="QQ" target="_blank" href="//wpa.qq.com/msgrd?v=3&uin=641904695&site=qq&menu=yes" rel="external nofollow"><i class="fa fa-qq fa-2x"></i></a> <a title="Weibo" target="_blank" href="//www.weibo.com/haoxiang969" rel="external nofollow"><i class="fa fa-weibo fa-2x"></i></a></div></nav></div><div id="noticeId" class="show" style="background:rgb(244,247,247,.3)"><marquee id="noticeMar" behavior="scoll" class="notice" direction="left" onmouseover="this.stop()" onmouseout="this.start()"></marquee></div><script type="text/javascript">function browserRedirect(){var i=navigator.userAgent.toLowerCase(),n="ipad"==i.match(/ipad/i),o="iphone os"==i.match(/iphone os/i),a="midp"==i.match(/midp/i),t="rv:1.2.3.4"==i.match(/rv:1.2.3.4/i),e="ucweb"==i.match(/ucweb/i),s="android"==i.match(/android/i),d="windows ce"==i.match(/windows ce/i),p="windows mobile"==i.match(/windows mobile/i),r="",c=randomColor(),r=n||o||a||t||e||s||d||p?($("#noticeId").css({"line-height":"1.6em"}),"<font style='color:"+c+"' face='新华宋体'  size='2'>官宣：<span id='weekend'></span><span id='msg'></span><span id='timer'></span><span id='hitokoto'></span></span></span><span id='YQData'></font> "):($("#noticeId").css({"line-height":"3.6em","margin-top":"20px"}),"<font style='color:"+c+"' face='新华宋体'  size='6'>官宣：<span id='weekend'></span><span id='msg'></span><span id='timer'></span><span id='hitokoto'></span></span><span id='YQData'></span></font> ");$("#noticeMar").html(r)}function randomColor(){for(var i="0,1,2,3,4,5,6,7,8,9,a,b,c,d,e,f".split(","),n="#",o=0;o<6;o++)n+=i[Math.floor(16*Math.random())];return n}$(function(){browserRedirect(),0<$("#hitokoto").length&&getHitokoto()})</script></div></header><script>console.log=function(){}</script><div class="outer"><section id="main" class="body-wrap"><article id="post-sync/-大规模训练-大规模训练的技术挑战" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><h1 class="post-title" itemprop="name">推荐系列- -大规模训练-大规模训练的技术挑战</h1><div class="post-title-bar"><ul><li><i class="fa fa-book"></i> <a href="/categories/热门文章/">热门文章</a></li><li><i class="fa fa-calendar"></i> 2022-05-11</li><li><i class="fa fa-eye"></i> <span id="busuanzi_value_page_pv"></span></li><li><span class="post-count">字数统计: 4.1k字</span></li><li><span class="post-count">阅读时长: 14分钟</span></li></ul></div></header><div class="article-entry post-content" itemprop="articleBody"><p>&amp;emsp;&amp;emsp;@ 不愿透露姓名的小 P 同学 0 前言 本次分享是大规模训练技术系列的第一篇，主要包括两个部分： 大规模训练技术的意义 大规模训练的技术挑战 1 大规模训练技术的意义 1.1 训练的精度极限 …</p><span id="more"></span><h4 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h4><p>本次分享是大规模训练技术系列的第一篇，主要包括两个部分：</p><p>大规模训练技术的意义<br>大规模训练的技术挑战</p><h4 id="1-大规模训练技术的意义"><a href="#1-大规模训练技术的意义" class="headerlink" title="1 大规模训练技术的意义"></a>1 大规模训练技术的意义</h4><h5 id="1-1-训练的精度极限"><a href="#1-1-训练的精度极限" class="headerlink" title="1.1 训练的精度极限"></a>1.1 训练的精度极限</h5><p>深度学习发展历程中，模型精度提升主要依赖神经网络在结构上的变革。<br>例如，从 AlexNet 到 ResNet50，再到 NAS 搜索出来的 EfficientNet，ImageNet Top-1 精度从 58 提升到了 84。<br>但是，随着神经网络结构设计技术逐渐成熟并趋于收敛，想要通过优化神经网络结构来打破精度局限是非常困难的。</p><h5 id="1-2-数据与模型规模对精度的影响"><a href="#1-2-数据与模型规模对精度的影响" class="headerlink" title="1.2 数据与模型规模对精度的影响"></a>1.2 数据与模型规模对精度的影响</h5><p>近年来，随着数据规模和模型规模的不断增大，模型精度也得到了进一步提升。一系列研究实验表明，从大模型与大数据入手提高模型精度是可行的。<br>一般的，构造大模型可以通过在现有的模型结构上进行加宽、加深、拼接得到。<br>更多的 featuremap 意味着更强的学习能力，因此对比相同结构的小模型，大模型有着更好的精度。以 Google 的 Big Transfer (BiT): General Visual Representation Learning[1]中的实验为例：</p><p>图中，在 ILSVRC-2012 数据集上训练不同规模的模型，精度能从 ResNet50（2600万参数）的 77%，提升到 ResNet152x4（2.4亿参数）的 81.3%，精度提升 4.3%。<br>类似的，扩大数据规模也能带来精度提升，如使用 ILSVRC-2012（128万张图片，1000个类别）和 JFT-300M（3亿张图片，18291个类别）两个数据集来训练 ResNet50，精度分别为 77% 和 79%。<br>可以发现，单独使用大数据集或者是大模型，均能带来一定的精度提升。<br>这些精度提升看起来比较有限，甚至在一些 case 下几乎没有提升，但是当我们同时使用大模型和大数据集时，则会有明显的突破。还是在上图中，使用 JFT-300M 训练 ResNet152x4，精度可以上升到 87.5%，相比 ILSVRC-2012+ResNet50 提升了 10.5%。<br>NLP 领域的大规模训练发展快速，从 BERT 到 GPT-3，再到 Switch Transformer，无论是模型大小还是计算资源占用都在疾速增长。规模大到什么程度呢？GPT-3 的参数量达到了 1750 亿，训练数据超过了 45TB，需要的算力 (flops) 是 BERT 的 1900 多倍，3.14E23 FLOPS。<br>换句话说，即使是在理论情况下（算力利用率 100%），使用单卡完整训练 GPT-3 也需要 V100 训练 355 年[4]。<br>目前 CNN 领域的模型参数规模普遍还在 1 亿以内。例如 EfficientNet-B7 也不过 6000 万参数。<br>但最新研究中，CNN 大模型也开始不断涌现。如 ResNeXt WSL（8亿参数）、GPipe（6亿参数），Google 也通过 EfficientNet-L2（4.8 亿参数）与未公开的 JFT-300M 数据集刷新了 ImageNet 的榜单，Top-1 Acc 首次突破 90。<br>可以看出，模型和数据规模的增大确实能突破现有的精度局限。</p><h5 id="1-3-从分布式训练到大规模训练"><a href="#1-3-从分布式训练到大规模训练" class="headerlink" title="1.3 从分布式训练到大规模训练"></a>1.3 从分布式训练到大规模训练</h5><p>模型和数据规模的增大意味着训练时间的增长。为了提升模型训练的速度，可以增加计算资源来缩短训练时间，于是出现了分布式训练。<br>简单来说，分布式训练实质就是将单卡的负载拆到了多卡上。</p><p>数据并行：通过修改 Sampler 切分输入，每张卡只需要处理一部分数据；<br>模型并行：通过修改层内的计算方式，将单层的计算负载和显存负载切分到多张卡上；<br>流水并行：将不同的层放到不同的卡上，进而将计算负载和显存负载切分至多张卡上。</p><p>通过对负载进行切分，分布式训练减少了单卡的负载，一方面大大提升了训练任务的吞吐量（切分计算负载），另一方面使得原本单卡无法训练的任务变得可能（切分显存负载）。<br>这里基于一个数据并行的例子进行更细致的分析：<br>假设深度学习模型，参数足够量小，能够放到单张 GPU 上；但是由于训练数据集相当大，所以整体任务的计算量非常大。<br>此时，数据并行的作用就可以体现了：对于这些深度学习训练来说，直接增大数据并行的计算资源是比较有效的，投入数倍的计算资源基本上能获取到相应的加速倍数。<br>但是随着训练数据集进一步增大，数据并行就会出现一些局限性：<br>当训练资源扩大到一定规模时（比如说上百卡），由于通信瓶颈的存在，添加资源的边际效应就会越来越明显，甚至增加再多资源都无法再加速。<br>更系统来说，随着数据和模型的不断增大，会触碰到两方面的问题。大数据和大模型所产生的显存墙问题（模型是否能跑起来）以及计算墙（能否在合理时间内完成训练）问题，使得普通的分布式训练不再适用于此类任务。</p><p>显存墙：单卡无法直接装下模型，模型无法直接训起来。为了能够将模型运行起来，需要使用模型并行、流水并行等技术，但是这些技术会降低 GPU 的运算强度。<br>计算墙：大数据+大模型意味着巨大的计算量，而由于显存墙的缘故，这时不仅单卡的运算强度低，多卡的加速比 (scale factor) 也非常差，事实就是，就算花钱加再多资源也可能训不完。</p><p>此时，需要引入大规模训练技术。<br>大规模训练技术在解决显存墙的同时，也不会被计算墙给拦住，以实现高效训练。</p><h4 id="2-大规模训练的技术挑战"><a href="#2-大规模训练的技术挑战" class="headerlink" title="2 大规模训练的技术挑战"></a>2 大规模训练的技术挑战</h4><p>相比普通的分布式训练，大规模训练技术考虑的问题更加复杂：</p><p>首先，面对单卡无法装载的大模型，如何利用���卡��突破显存瓶颈是个问题；<br>其次，大规模训练会用到大量的计算资源，大量计算资源间如何通信、协作是另一个难题；<br>最后，如何 balance 各类层出不穷的大规模训练技术，使得众多技术形成一个完整高效的训练方案，更是一大学问。</p><p>本系列中，将大规模训练技术面临的挑战分为三部分：显存、通信和计算，每部分都会有对应文章进行详细介绍。<br>其中，显存部分主要阐述大规模训练中如何解决显存墙问题，其它两部分则是为了解决计算墙问题。</p><h5 id="2-1-大规模训练之显存挑战"><a href="#2-1-大规模训练之显存挑战" class="headerlink" title="2.1 大规模训练之显存挑战"></a>2.1 大规模训练之显存挑战</h5><p>大模型训练最先遇到的问题就是显存墙。<br>事实上，在进行 ResNeSt269（1亿参数）的 ImageNet 训练时，显存就已经逼近了 V100 32GB 的上限，训练占用达到了 28GB（注：Batch size 为 16，Input size 为 416×416）。当模型进一步加大，或者加大 Batch size 后，模型训练的显存占用也会随之增长，最后高于显卡的显存容量，触碰到了显存墙，模型无法训练。<br>要理解显存墙问题的本质，就需要了解显存增长的本质。下图是 ResNeSt269 在 1 轮迭代间的显存占用变化：</p><p>Forward 开始后，显存占用不断飙升，在 Forward 结束后达到峰值 28GB。<br>Forward 结束，Backward 开始后，显存占用不断下降，在 Backward 结束后为 1.2GB。<br>Backward 结束后，仍有一部分显存驻留在显卡中，大小保持在 1.2GB。</p><p>从图中可以分析出一些有趣的信息：<br>第一，可以发现，模型训练对显存的占用可以分为两部分：一部分是模型 forward 时保存下来的临时变量，这部分显存会在反向传播时会逐渐释放掉，这部分一般被称为 Activations。另一部分则是参数、梯度等状态信息占用的显存，这部分一般被称为 Model States。<br>第二，Forward 结束后的显存占用峰值时刻决定了是否会碰到显存墙。降低其余部分的显存占用没有意义，关键在于削低峰值。<br>接下来具体介绍一下峰值时的 Model states 和 Activations。</p><h6 id="2-1-1-Model-states"><a href="#2-1-1-Model-states" class="headerlink" title="2.1.1 Model states"></a>2.1.1 Model states</h6><p>虽然在 ResNeSt269 的 case 里 Model states 仅占用了 1.2GB。但也要考虑到 ResNeSt269 仅 1 亿参数，和当前存在的大模型相比还相差甚远。<br>但实际上大模型 Model states 的占用非常恐怖，通过以下公式就能够感受到这一点。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">  P：模型参数量，单位为Billion</span><br><span class="line"></span><br><span class="line">当优化器是 SGD 时，占用大小为：</span><br><span class="line">MS_FP16 = 2P（FP16参数）+2P（FP16梯度）+8P（FP32的参数、动量） = 12<span class="type">P</span></span><br><span class="line"><span class="variable">MS_FP32</span> <span class="operator">=</span> 12P（FP32的参数、动量、梯度） = 12P</span><br><span class="line"></span><br><span class="line">当优化器是 ADAM 时，占用大小为：</span><br><span class="line">MS_FP16 = 2P（FP16参数)+2P（FP16梯度）+12P(FP32的参数、动量、variances) = 16<span class="type">P</span></span><br><span class="line"><span class="variable">MS_FP32</span> <span class="operator">=</span> 16P（FP32的参数、动量、梯度、variances） = 16P</span><br><span class="line"></span><br><span class="line">SGD 下，MS_FP32_ResNeSt269 = <span class="number">0.1</span> * <span class="number">12</span> = <span class="number">1.2</span> GB</span><br><span class="line">ADAM 下，MS_FP16_GPT-<span class="number">3</span> = <span class="number">170</span> * <span class="number">16</span> = <span class="number">2720</span> GB</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>因此，一张 V100 32GB 最多能放下参数量为 20+ 亿的模型。不过就算能放下模型，也没有意义，因为已经没有剩余显存留给 Activations 了。</p><h6 id="2-1-2-Activations"><a href="#2-1-2-Activations" class="headerlink" title="2.1.2 Activations"></a>2.1.2 Activations</h6><p>Activations 一般指的是用于 autograd 的中间变量。如上表[3]中的Forward Output、Output Gradient 以及 cuDNN Workspace。<br>因为深度神经网络层数非常多，而每一层的 Forward Output 都需要保存下来给 Backward 用，由此累积下来会造成巨大的内存开销。从 ResNeSt269 中的 case 我们也可以大致看出，Activations 的占用要远远大于 Model States（1.2GB vs 26.8GB）。<br>事实正是如此，CNN 模型中的 Activations 占了显存的大头， Activations 对 CNN 模型来说是一个非常棘手的问题。<br>而另一类模型：Transformer 模型的情况则相对更好。原因主要在于 Transformer 中的内部运算以大矩阵乘居多，而大矩阵乘可以拆分做模型并行（CNN 的模型并行非常难），大大减少了 Activations 的占用。<br>这也是为什么近年来基于 Transformer 的大模型层出不穷，很快模型规模就到了千亿级别，而基于 CNN 的大模型还在亿级。<br>Model states 和 Activations 都有可能造成显存墙问题。它们相互独立但又相互制约。任意一侧的增大都会导致留给另一侧的显存空间变小，所以单单对一侧做优化是不够的，必须同时优化 Model states 和 Activations。</p><h5 id="2-2-大规模训练之通信挑战"><a href="#2-2-大规模训练之通信挑战" class="headerlink" title="2.2 大规模训练之通信挑战"></a>2.2 大规模训练之通信挑战</h5><p>在进行分布式训练时，我们对神经网络进行了各种各样的切分，但是神经网络的训练任务仍是一个整体。因此，切分需要通信来进行聚合。<br>聚合所产生的通信需求隐含了不少问题。首先，深度学习迭代式训练的特性导致更新频繁，需要大量的交换局部更新。但是，目前网络的传输速率远远不能匹配 GPU 或 TPU 这种专用加速芯片的运算速率。<br>有的朋友可能会问，不能直接用更大的带宽来解决这些问题吗？<br>答案是 NO！。直接增加通信带宽并不是一个有效的解决方案[2]：</p><p>从图中可以看出，随着带宽增大，带宽利用率将越来越低，高带宽的增益非常有限。实际上，其根本原因在于网络协议栈——网络协议栈的开销导致了训练无法有效利用带宽。幸好，我们能够通过一些方法（会在对应文章中详细阐述）解决掉这部分开销。<br>但是，即使解决了协议栈的开销，面对大规模的训练任务时，仍然会出现一些新的问题：</p><p>随着机器规模的扩大，基于 Ring-AllReduce 的通信聚合方式所构造的 Ring 将越来越大，延迟将不可接受。<br>模型规模的扩大也造成了通信量的剧烈增长：</p><p>模型变大导致所需通信的梯度变多，带宽还是扛不住；<br>为了训练大模型而引入的模型并行、流水并行等大大增加了通信压力。</p><p>目前普遍采用了同步的通信步调，成百上千卡频繁的进行同步非常容易出现水桶效应，导致单卡上的波动以及单次通信的延迟被疯狂放大。</p><p>总的来说，大规模深度学习中的通信瓶颈不单单是通信量大造成的，更多的是复杂的系统问题，还需要从分布式训练架构、通信策略等多方面来考虑解决。</p><h5 id="2-3-大规模训练之计算挑战"><a href="#2-3-大规模训练之计算挑战" class="headerlink" title="2.3 大规模训练之计算挑战"></a>2.3 大规模训练之计算挑战</h5><p>大模型+大数据不仅带来了极高的算力需求，同时也在引入各项技术的同时降低了计算资源的利用率。在我们的一项内部实验中，优化前使用 128 卡进行 25 亿参数量 CNN 模型训练的耗时预估达到了 473.8 天。<br>虽然算力挑战本质来源于大规模训练任务庞大的算力需求，但是一般来说，我们无法直接��少任务的算力需求，因此只能从提高计算效率来考虑。<br>计算效率问题自底向上可以分为 Operator-level、Graph-level 以及 Task-level 三个层面。</p><h6 id="2-3-1-Operator-level"><a href="#2-3-1-Operator-level" class="headerlink" title="2.3.1 Operator-level"></a>2.3.1 Operator-level</h6><p>Operator-level 可以理解为算子级别的优化。<br>大规模训练中的 Operator-level 问题与单卡训练类似。<br>如：</p><p>小算子过多；<br>Kernel 实现不够高效；<br>内存局部性差。</p><h6 id="2-3-2-Graph-level"><a href="#2-3-2-Graph-level" class="headerlink" title="2.3.2 Graph-level"></a>2.3.2 Graph-level</h6><p>Graph-level 指的是如何对计算图进行优化，进而加速大规模训练。<br>如：</p><p>如何搜索出计算效率更高的计算图；<br>如何用计算编译技术解决小算子问题；<br>如何进行通信和计算的 overlap 等。</p><h6 id="2-3-3-Task-level"><a href="#2-3-3-Task-level" class="headerlink" title="2.3.3 Task-level"></a>2.3.3 Task-level</h6><p>可以理解为训练阶段的系统设计。<br>与传统训练不同，在包含大规模训练技术的训练系统设计时，不仅要考虑到庞大的节点数，也要考虑到在解决显存、通信问题时带来的系统层面的变化。因此，Task-level 的关键挑战在于，如何给出一个最终计算效率最高的系统设计。<br>如：</p><p>采用何种分布式训练架构，才能使得大规模训练具备良好的拓展性。在节点数很多时仍能保持较好的加速比（scale factor）；<br>如何 balance 显存优化与速度优化。</p><p>所以，大规模训练中的算力问题是一个综合问题，无法用单一的技术进行解决，而是需要一个整体的解决方案。这个解决方案不仅需要拥有足够的计算资源，也依赖于深度学习框架的运行效率，以及对各项大规模训练优化的 trade off。</p><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[1]: Kolesnikov A, Beyer L, Zhai X, et al. Big transfer (bit): General visual representation learning[J]. arXiv preprint arXiv:1912.11370, 2019, 6(2): 8.<br>[2]: Zhang, Zhen, et al. “Is network the bottleneck of distributed training?.” Proceedings of the Workshop on Network Meets AI &amp; ML. 2020.<br>[3]: Gao Y, Liu Y, Zhang H, et al. Estimating gpu memory consumption of deep learning models[C]&#x2F;&#x2F;Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2020: 1342-1352.<br>[4]: OpenAI’s GPT-3 Language Model: A Technical Overview</p><p>感谢阅读，欢迎在评论区留言讨论哦~<br>P.S. 如果喜欢本篇文章，请多多 点赞，让更多的人看见我们 :D</p><div class="post-copyright"><div class="content"><p>本文标题： 推荐系列- -大规模训练-大规模训练的技术挑战</p><p>本文作者： OSChina</p><p>发布时间： 2022年05月11日 05:14</p><p>最后更新： 2022年05月11日 17:37</p><p>原始链接： <a class="post-url" href="/c3019c25/" title="推荐系列- -大规模训练-大规模训练的技术挑战">https://www.hosiang.cn/c3019c25/</a></p><p>版权声明： 本文著作权归作者所有，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a>许可协议，转载请注明出处！</p><footer><a href="https://www.hosiang.cn"><img src="/../images/logo.png" alt="Howe Hsiang"> Howe Hsiang</a></footer></div></div><div class="page-reward"><a id="rewardBtn" href="javascript:;">赏</a></div><div id="reward" class="post-modal reward-lay"><a class="close" href="javascript:;" id="reward-close">×</a> <span class="reward-title"><i class="icon icon-quote-left"></i> 喜欢就赞赏一下呗！ <i class="icon icon-quote-right"></i></span><div class="reward-content"><div class="reward-code"><img id="rewardCode" src="/images/threepay_code.jpg" alt="打赏二维码"></div><div class="reward-select"></div></div></div></div><footer class="article-footer"><div class="post-share"><a href="javascript:" id="share-sub" class="post-share-fab"><i class="fa fa-share-alt" style="padding-top:11px!important"></i></a><div class="post-share-list" id="share-list"><ul class="share-icons"><li><a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://www.hosiang.cn/c3019c25/" data-title="Facebook" rel="external nofollow noopener noreferrer"><i class="fa fa-facebook" style="padding-top:9px!important"></i></a></li><li><a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《推荐系列- -大规模训练-大规模训练的技术挑战》 — 狂欢马克思&url=https://www.hosiang.cn/c3019c25/&via=https://www.hosiang.cn" data-title="Twitter" rel="external nofollow noopener noreferrer"><i class="fa fa-twitter" style="padding-top:9px!important"></i></a></li><li><a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://www.hosiang.cn/c3019c25/" data-title="Google+" rel="external nofollow noopener noreferrer"><i class="fa fa-google-plus" style="padding-top:9px!important"></i></a></li><li><a class="qq share-sns" target="_blank" href="https://connect.qq.com/widget/shareqq/index.html?url=https://www.hosiang.cn/c3019c25/&title=《推荐系列- -大规模训练-大规模训练的技术挑战》 — 狂欢马克思&source=&amp;emsp;&amp;emsp;@ 不愿透露姓名的小 P 同学 0 前言 本次分享是大规模训练技术系列的第一篇，主要包括两个部分： 大规模训练技术..." data-title="QQ" rel="external nofollow noopener noreferrer"><i class="fa fa-qq" style="padding-top:9px!important"></i></a></li><li><a class="weixin share-sns" id="wxFab" href="javascript:" data-title="微信"><i class="fa fa-weixin" style="padding-top:9px!important"></i></a></li><li><a class="weibo share-sns" target="_blank" href="https://service.weibo.com/share/share.php?url=https://www.hosiang.cn/c3019c25/&title=《推荐系列- -大规模训练-大规模训练的技术挑战》 — 狂欢马克思&pic=https://oscimg.oschina.net/oscnet/up-b250ba4ca511e6b04b006de961d98ed6591.jpg" data-title="微博" rel="external nofollow noopener noreferrer"><i class="fa fa-weibo" style="padding-top:9px!important"></i></a></li></ul></div></div><div class="post-modal wx-share" id="wxShare"><a class="close" href="javascript:" id="wxShare-close">×</a><p>扫一扫，分享到微信</p><img src="//api.qrserver.com/v1/create-qr-code/?data=https://www.hosiang.cn/c3019c25/" alt="微信分享二维码"></div><div class="mask"></div><ul class="article-footer-menu"><li class="article-footer-tags"><i class="fa fa-tags"></i> <a href="/tags/Popular/" class="color3">Popular</a></li></ul></footer></div></article><aside class="post-toc-pos post-toc-top" id="post-toc"><nav class="post-toc-wrap"><ol class="post-toc"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#0-%E5%89%8D%E8%A8%80"><span class="post-toc-text">0 前言</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1-%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E7%9A%84%E6%84%8F%E4%B9%89"><span class="post-toc-text">1 大规模训练技术的意义</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#1-1-%E8%AE%AD%E7%BB%83%E7%9A%84%E7%B2%BE%E5%BA%A6%E6%9E%81%E9%99%90"><span class="post-toc-text">1.1 训练的精度极限</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#1-2-%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%A7%84%E6%A8%A1%E5%AF%B9%E7%B2%BE%E5%BA%A6%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="post-toc-text">1.2 数据与模型规模对精度的影响</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#1-3-%E4%BB%8E%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%88%B0%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%AE%AD%E7%BB%83"><span class="post-toc-text">1.3 从分布式训练到大规模训练</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%AE%AD%E7%BB%83%E7%9A%84%E6%8A%80%E6%9C%AF%E6%8C%91%E6%88%98"><span class="post-toc-text">2 大规模训练的技术挑战</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#2-1-%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%AE%AD%E7%BB%83%E4%B9%8B%E6%98%BE%E5%AD%98%E6%8C%91%E6%88%98"><span class="post-toc-text">2.1 大规模训练之显存挑战</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#2-1-1-Model-states"><span class="post-toc-text">2.1.1 Model states</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#2-1-2-Activations"><span class="post-toc-text">2.1.2 Activations</span></a></li></ol></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#2-2-%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%AE%AD%E7%BB%83%E4%B9%8B%E9%80%9A%E4%BF%A1%E6%8C%91%E6%88%98"><span class="post-toc-text">2.2 大规模训练之通信挑战</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#2-3-%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%AE%AD%E7%BB%83%E4%B9%8B%E8%AE%A1%E7%AE%97%E6%8C%91%E6%88%98"><span class="post-toc-text">2.3 大规模训练之计算挑战</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#2-3-1-Operator-level"><span class="post-toc-text">2.3.1 Operator-level</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#2-3-2-Graph-level"><span class="post-toc-text">2.3.2 Graph-level</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#2-3-3-Task-level"><span class="post-toc-text">2.3.3 Task-level</span></a></li></ol></li></ol></li></ol><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#References"><span class="post-toc-text">References</span></a></li></nav></aside><nav id="article-nav"><a href="/18658a1f/" id="article-nav-newer" class="article-nav-link-wrap"><span class="article-nav-title"><i class="fa fa-hand-o-left" aria-hidden="true"></i> 推荐系列--上游优先-，与善良无关 </span></a><a href="/3aa85cd3/" id="article-nav-older" class="article-nav-link-wrap"><span class="article-nav-title">推荐系列--高手问答 281 期汇总- —— SaaS攻略-从入门到进阶</span> <i class="fa fa-hand-o-right" aria-hidden="true"></i></a></nav><div id="gitalk"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script>var gitalk=new Gitalk({repo:"bolg-comment",owner:"Hosiang1026",admin:"Hosiang1026",clientID:"37a2dd4473e8970cecc2",clientSecret:"e30efa17d86d9de4f4ab810872dc62a7b2e7e634",pagerDirection:"last",distractionFreeMode:!0,createIssueManually:!1});gitalk.render("gitalk")</script></section></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner"><script id="_wau78w">var _wau=_wau||[];_wau.push(["small","5dnguv4c2n","78w"])</script><script async src="//waust.at/s.js"></script><p><span id="busuanzi_container_site_uv" style="display:none"><span class="post-count">总字数：<span>1781.1k</span> </span>总访客数：<span id="busuanzi_value_site_uv"></span> </span><span id="busuanzi_container_site_pv" style="display:none">总访问量：<span id="busuanzi_value_site_pv"></span></span><br><span id="TimeShow">本站</span><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><br>Copyright&copy; 2018 - 2022 狂欢马克思 <a href="https://beian.miit.gov.cn" rel="external nofollow noopener noreferrer" target="_blank">京ICP备17060439号</a></p></div></div><script>var now=new Date;function createtime(){var n=new Date("2017/09/18");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="安全运行 "+dnum+" 天 ",document.getElementById("times").innerHTML=hnum+" 小时 "+mnum+" 分 "+snum+" 秒"}setInterval("createtime()",800)</script></footer><script>var mihoConfig={root:"https://www.hosiang.cn",animate:"true",isHome:"false",share:"true",reward:" 1"}</script><div class="sidebar"><div id="sidebar-search" title="Search"><i class="fa fa-search"></i></div><div id="sidebar-category" title="Categories"><i class="fa fa-book"></i></div><div id="sidebar-tag" title="Tags"><i class="fa fa-tags"></i></div><div id="sidebar-top"><span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span></div></div><div class="sidebar-menu-box" id="sidebar-menu-box"><div class="sidebar-menu-box-container"><div id="sidebar-menu-box-categories"><a class="category-link" href="/categories/%E5%85%B4%E8%B6%A3%E7%88%B1%E5%A5%BD/">兴趣爱好</a><a class="category-link" href="/categories/%E5%85%B6%E4%BB%96%E5%BC%80%E5%8F%91/">其他开发</a><a class="category-link" href="/categories/%E5%89%8D%E6%B2%BF%E5%BC%80%E5%8F%91/">前沿开发</a><a class="category-link" href="/categories/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/">前端开发</a><a class="category-link" href="/categories/%E5%8D%87%E7%BA%A7%E7%89%88%E6%9C%AC/">升级版本</a><a class="category-link" href="/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/">博客教程</a><a class="category-link" href="/categories/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/">后端开发</a><a class="category-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a><a class="category-link" href="/categories/%E6%97%A5%E5%B8%B8%E8%AE%B0%E5%BD%95/">日常记录</a><a class="category-link" href="/categories/%E7%83%AD%E9%97%A8%E6%96%87%E7%AB%A0/">热门文章</a></div><div id="sidebar-menu-box-tags"><a href="/tags/Ajax/" style="font-size:10px">Ajax</a> <a href="/tags/AliGenie/" style="font-size:10px">AliGenie</a> <a href="/tags/Alipay/" style="font-size:10px">Alipay</a> <a href="/tags/Android/" style="font-size:10px">Android</a> <a href="/tags/Blockchain/" style="font-size:17.5px">Blockchain</a> <a href="/tags/CQRS/" style="font-size:10px">CQRS</a> <a href="/tags/Database/" style="font-size:15px">Database</a> <a href="/tags/Docker/" style="font-size:10px">Docker</a> <a href="/tags/EasyUI/" style="font-size:10px">EasyUI</a> <a href="/tags/Guitar/" style="font-size:10px">Guitar</a> <a href="/tags/Hackintosh/" style="font-size:12.5px">Hackintosh</a> <a href="/tags/Hexo/" style="font-size:15px">Hexo</a> <a href="/tags/IntelliJ-IDEA/" style="font-size:10px">IntelliJ IDEA</a> <a href="/tags/Java/" style="font-size:12.5px">Java</a> <a href="/tags/Kali-Linux/" style="font-size:10px">Kali Linux</a> <a href="/tags/Life/" style="font-size:17.5px">Life</a> <a href="/tags/Mac-OS/" style="font-size:12.5px">Mac OS</a> <a href="/tags/MultiThread/" style="font-size:10px">MultiThread</a> <a href="/tags/NodeJS/" style="font-size:10px">NodeJS</a> <a href="/tags/Popular/" style="font-size:20px">Popular</a> <a href="/tags/Python/" style="font-size:10px">Python</a> <a href="/tags/SaaS/" style="font-size:10px">SaaS</a> <a href="/tags/Servlet/" style="font-size:10px">Servlet</a> <a href="/tags/Spring/" style="font-size:17.5px">Spring</a> <a href="/tags/Upgrade/" style="font-size:10px">Upgrade</a> <a href="/tags/Web/" style="font-size:10px">Web</a> <a href="/tags/Windows/" style="font-size:10px">Windows</a> <a href="/tags/Work/" style="font-size:10px">Work</a> <a href="/tags/iOS/" style="font-size:10px">iOS</a></div></div><a href="javascript:" class="sidebar-menu-box-close">&times;</a></div><div class="mobile-header-menu-nav" id="mobile-header-menu-nav"><div class="mobile-header-menu-container"><span class="title">导航</span><ul class="mobile-header-menu-navbar"><li><a href="/"><i class="fa fa-home"></i><span>主页</span></a></li><li><a href="/archives"><i class="fa fa-archive"></i><span>归档</span></a></li><li><a href="/gitbook"><i class="fa fa-columns"></i><span>笔记</span></a></li><li><a href="/music"><i class="fa fa-music"></i><span>音乐</span></a></li><li><a href="/photo"><i class="fa fa-picture-o"></i><span>相册</span></a></li><li><a href="/collection"><i class="fa fa-envira"></i><span>收藏</span></a></li><li><a href="/about"><i class="fa fa-user"></i><span>关于</span></a></li></ul></div><div class="mobile-header-tag-container"><span class="title">标签</span><div id="mobile-header-container-tags"><a href="/tags/Ajax/" style="font-size:10px">Ajax</a> <a href="/tags/AliGenie/" style="font-size:10px">AliGenie</a> <a href="/tags/Alipay/" style="font-size:10px">Alipay</a> <a href="/tags/Android/" style="font-size:10px">Android</a> <a href="/tags/Blockchain/" style="font-size:17.5px">Blockchain</a> <a href="/tags/CQRS/" style="font-size:10px">CQRS</a> <a href="/tags/Database/" style="font-size:15px">Database</a> <a href="/tags/Docker/" style="font-size:10px">Docker</a> <a href="/tags/EasyUI/" style="font-size:10px">EasyUI</a> <a href="/tags/Guitar/" style="font-size:10px">Guitar</a> <a href="/tags/Hackintosh/" style="font-size:12.5px">Hackintosh</a> <a href="/tags/Hexo/" style="font-size:15px">Hexo</a> <a href="/tags/IntelliJ-IDEA/" style="font-size:10px">IntelliJ IDEA</a> <a href="/tags/Java/" style="font-size:12.5px">Java</a> <a href="/tags/Kali-Linux/" style="font-size:10px">Kali Linux</a> <a href="/tags/Life/" style="font-size:17.5px">Life</a> <a href="/tags/Mac-OS/" style="font-size:12.5px">Mac OS</a> <a href="/tags/MultiThread/" style="font-size:10px">MultiThread</a> <a href="/tags/NodeJS/" style="font-size:10px">NodeJS</a> <a href="/tags/Popular/" style="font-size:20px">Popular</a> <a href="/tags/Python/" style="font-size:10px">Python</a> <a href="/tags/SaaS/" style="font-size:10px">SaaS</a> <a href="/tags/Servlet/" style="font-size:10px">Servlet</a> <a href="/tags/Spring/" style="font-size:17.5px">Spring</a> <a href="/tags/Upgrade/" style="font-size:10px">Upgrade</a> <a href="/tags/Web/" style="font-size:10px">Web</a> <a href="/tags/Windows/" style="font-size:10px">Windows</a> <a href="/tags/Work/" style="font-size:10px">Work</a> <a href="/tags/iOS/" style="font-size:10px">iOS</a></div></div></div><div class="search-wrap"><span class="search-close">&times;</span> <a href="javascript:" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg icon-chevron-left"></i> </a><input class="search-field" placeholder="Search..." id="keywords"> <a id="search-submit" href="javascript:"><i class="fa fa-search"></i></a><div class="search-container" id="search-container"><ul class="search-result" id="search-result"></ul></div></div><div id="search-tpl"><li class="search-result-item"><a href="{url}" class="search-item-li"><span class="search-item-li-title" title="{title}">{title}</span></a></li></div><script src="/js/search.js"></script><script src="/js/main.js"></script><script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script><div id="particles"></div><script src="/js/particles.js"></script><link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css"><script src="//cdn.bootcss.com/scrollReveal.js/3.0.5/scrollreveal.js"></script><script src="/js/animate.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/hijiki.model.json"},display:{position:"right",width:150,height:300},mobile:{show:!0,scale:.5},log:!1})</script></body></html>